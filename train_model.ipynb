{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef074001",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6c36abe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading cleaned version of Dataset\n",
    "\n",
    "df = pd.read_csv('cleaned_loan_default.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab1dfbe8",
   "metadata": {},
   "source": [
    "## Encoding Categorical Columns\n",
    "\n",
    "I have convert all categorical columns into numerical values using one hot encoding.\n",
    "This is required because machine learning models such as Logistic Regression and Decision Tree cannot work with text labels.\n",
    "\n",
    "I am using pd.get_dummies() to ensure safe and simple encoding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bdfd8bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dropping the ID column because it is not useful in prediction\n",
    "\n",
    "df = df.drop(columns='ID')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2e5d430",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encoding Features\n",
    "\n",
    "df_encoded = pd.get_dummies(df, drop_first=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1fca4a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_encoded.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b667d22b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_encoded.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75848663",
   "metadata": {},
   "source": [
    "## Train–Test Split\n",
    "\n",
    "I have separate the data into features (X) and target (y) and then split it into training and testing sets.\n",
    "The training set is used to fit the model, and the test set is used to evaluate how well the model generalizes to unseen data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9eb6b3fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df_encoded.drop(columns='Status', axis=1)\n",
    "Y = df_encoded['Status']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0ac9a5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, random_state=42, stratify=Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54a9f20c",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.shape, X_test.shape, Y_train.shape, Y_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49169e2d",
   "metadata": {},
   "source": [
    "## Train Logistic Regression Model\n",
    "\n",
    "I have train a Logistic Regression model to predict loan default.\n",
    "This algorithm works well for binary classification problems like default vs non default.\n",
    "I have first scale the numeric features for better performance then fit the model and generate predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d137860e",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "log_reg = LogisticRegression(max_iter=1000)\n",
    "log_reg.fit(X_train_scaled, Y_train)\n",
    "\n",
    "Y_pred_log = log_reg.predict(X_test_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "beb7f75f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation\n",
    "\n",
    "# Confusion Matrix:\n",
    "cm_log = confusion_matrix(Y_test, Y_pred_log)\n",
    "\n",
    "# Confusion Matrix HeatMap:\n",
    "plt.figure(figsize=(6,4))\n",
    "sns.heatmap(cm_log, annot=True, fmt='d', cmap='Blues')\n",
    "plt.title(\"Confusion Matrix - Logistic Regression\")\n",
    "plt.xlabel(\"Predicted\")\n",
    "plt.ylabel(\"Actual\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fac3c86",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Report & Accuracy\n",
    "\n",
    "log_reg_accuracy = accuracy_score(Y_test, Y_pred_log)\n",
    "print(\"Logistic Regression Accuracy:\", log_reg_accuracy)\n",
    "\n",
    "print(\"Classification Report - Logistic Regression:\")\n",
    "print(classification_report(Y_test, Y_pred_log))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80a4663e",
   "metadata": {},
   "source": [
    "## Train Decision Tree Classifier\n",
    "\n",
    "I have train a Decision Tree classifier as well on the same train test split.\n",
    "Decision Trees can capture non linear patterns and don’t require feature scaling so that's why I train this as well to see which model performs better.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c7b7668",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "dt = DecisionTreeClassifier(random_state=42, max_depth=5)\n",
    "\n",
    "dt.fit(X_train, Y_train)\n",
    "\n",
    "y_pred_dt = dt.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "519d6756",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation\n",
    "\n",
    "# Confusion Matrix\n",
    "\n",
    "cm_dt = confusion_matrix(Y_test, y_pred_dt)\n",
    "\n",
    "plt.figure(figsize=(6,4))\n",
    "sns.heatmap(cm_dt, annot=True, fmt='d', cmap='Greens')\n",
    "plt.title(\"Confusion Matrix - Decision Tree\")\n",
    "plt.xlabel(\"Predicted\")\n",
    "plt.ylabel(\"Actual\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80d00924",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classification Report\n",
    "\n",
    "print(\"Classification Report - Decision Tree:\")\n",
    "print(classification_report(Y_test, y_pred_dt))\n",
    "\n",
    "# Accuracy\n",
    "dt_accuracy = accuracy_score(Y_test, y_pred_dt)\n",
    "print(\"Decision Tree Accuracy:\", dt_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e62bf80",
   "metadata": {},
   "source": [
    "## Accuracy Comparison \n",
    "\n",
    "I have compare Logistic Regression and Decision Tree based on their accuracy using a bar chart."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd71ec93",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Accuracy Comparison Bar Plot\n",
    "model_names = ['Logistic Regression', 'Decision Tree']\n",
    "accuracies = [log_reg_accuracy, dt_accuracy]\n",
    "\n",
    "plt.figure(figsize=(7,5))\n",
    "sns.barplot(x=model_names, y=accuracies)\n",
    "plt.title(\"Model Accuracy Comparison\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.ylim(0,1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef453c16",
   "metadata": {},
   "source": [
    "Both models performed well but the Decision Tree achieved slightly higher accuracy proving that the relationships in this dataset are non linear and better captured by a tree based model.\n",
    "However, Logistic Regression still delivered strong performance and gives more interpretable coefficients."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
